{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pd_helpers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup \u001b[38;5;28;01mas\u001b[39;00m bs\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpd_helpers\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpdh\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     11\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pd_helpers'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "import pd_helpers as pdh\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as \n",
    "def pull_bea():\n",
    "    ### Make u_employment.csv\n",
    "    geofips = 'COUNTY'\n",
    "    linecode = '300' # Private nonfarm earnings: Utilities (22)\n",
    "    # linecode = '905' # Private nonfarm earnings: Telecommunications (517) # not county-level\n",
    "    # linecode = '800' # Private nonfarm earnings: Transportation and warehousing (48-49)\n",
    "    # linecode = '811' # Private nonfarm earnings: Warehousing and storage (493) # not county-level\n",
    "    tablename = 'CAEMP25N' # Total Full-Time and Part-Time Employment by NAICS Industry\n",
    "    call = f'https://apps.bea.gov/api/data/?&UserID=973C0B87-9C21-4559-BF3D-AB7584B1445C&method=getdata&datasetname=Regional&geofips=%s&linecode=%s&tablename=%s&Year=ALL&ResultFormat=XML' % (geofips, linecode, tablename)\n",
    "\n",
    "    call_xml = requests.get(call).text\n",
    "    call_soup = bs(call_xml, 'lxml')\n",
    "    call_list = [[line['geoname'], line['timeperiod'], line['datavalue']] for line in call_soup.find_all('data')]\n",
    "    call_df = pd.DataFrame(data=call_list, columns=['County', 'Year', 'Number of Employees'])\n",
    "    call_df = call_df.astype({'County': str, 'Year': int, 'Number of Employees': int})\n",
    "    udf = call_df.groupby('County')['Number of Employees'].last() # use last year\n",
    "    udf.to_csv('u_employment.csv')\n",
    "\n",
    "    ### Make tw_compensation.csv\n",
    "    geofips = 'COUNTY'\n",
    "    # linecode = '300' # Private nonfarm earnings: Utilities (22)\n",
    "    # linecode = '905' # Private nonfarm earnings: Telecommunications (517)\n",
    "    linecode = '800' # Private nonfarm earnings: Transportation and warehousing (48-49)\n",
    "    # linecode = '811' # Private nonfarm earnings: Warehousing and storage (493)\n",
    "    # tablename = 'CAINC5N' # Personal Income by Major Component and Earnings by NAICS Industry\n",
    "    tablename = 'CAINC6N' # Compensation of Employees by NAICS Industry \n",
    "    call = f'https://apps.bea.gov/api/data/?&UserID=973C0B87-9C21-4559-BF3D-AB7584B1445C&method=getdata&datasetname=Regional&geofips=%s&linecode=%s&tablename=%s&Year=ALL&ResultFormat=XML' % (geofips, linecode, tablename)\n",
    "\n",
    "    call_xml = requests.get(call).text\n",
    "    call_soup = bs(call_xml, 'lxml')\n",
    "    call_list = [[line['geoname'], line['timeperiod'], line['datavalue']] for line in call_soup.find_all('data')]\n",
    "    call_df = pd.DataFrame(data=call_list, columns=['County', 'Year', 'Dollars (Thousands)'])\n",
    "    call_df = call_df.astype({'County': str, 'Year': int, 'Dollars (Thousands)': float})\n",
    "\n",
    "    geofips = 'COUNTY'\n",
    "    linecode = '800'\n",
    "    tablename = 'CAEMP25N' # Total Full-Time and Part-Time Employment by NAICS Industry\n",
    "    call = f'https://apps.bea.gov/api/data/?&UserID=973C0B87-9C21-4559-BF3D-AB7584B1445C&method=getdata&datasetname=Regional&geofips=%s&linecode=%s&tablename=%s&Year=ALL&ResultFormat=XML' % (geofips, linecode, tablename)\n",
    "\n",
    "    call_xml = requests.get(call).text\n",
    "    call_soup1 = bs(call_xml, 'lxml')\n",
    "    call_list1 = [[line['geoname'], line['timeperiod'], line['datavalue']] for line in call_soup1.find_all('data')]\n",
    "    call_df1 = pd.DataFrame(data=call_list1, columns=['County', 'Year', 'Number of Employees'])\n",
    "    call_df1 = call_df1.astype({'County': str, 'Year': int, 'Number of Employees': float})\n",
    "\n",
    "    df = pd.merge(left=call_df, right=call_df1, on=['County', 'Year'], how='inner')\n",
    "    last_yr = df['Year'].max()\n",
    "    df = df[df['Year'] >= last_yr - 5] # last 5 years\n",
    "    df['Average Compensation'] = df['Dollars (Thousands)'] * 1000 / df['Number of Employees']\n",
    "    df.replace(0, np.nan, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    comp = df.groupby('County').mean()['Average Compensation'] # use average of last 5 years\n",
    "    employ = df.groupby('County').last()['Number of Employees'] # use last non-zero\n",
    "    df1 = pd.DataFrame([comp, employ]).transpose()\n",
    "    df1.rename(columns={'Number of Employees': f'Number of Employees %i' % last_yr}, inplace=True)\n",
    "    df1.to_csv('tw_compensation.csv')\n",
    "\n",
    "def make_tw_comp_by_zip(tw_comp_path='tw_compensation.csv', zip_info_path='zip_census.csv', min_comp=25000, max_comp=65000):\n",
    "    bea = pd.read_csv(tw_comp_path, index_col=0)\n",
    "\n",
    "    zip_info = pd.read_csv(zip_info_path, dtype={'zip': str})\n",
    "    zip_info = zip_info[~zip_info['military']]\n",
    "    zip_info = zip_info[~zip_info['state_id'].isin(['AK', 'HI', 'DC','GU','PR','VI','MP','AS'])]\n",
    "    zip_info = zip_info[['zip', 'lat', 'lng', 'county_name', 'state_id', 'county_names_all']]\n",
    "    zip_info['county_names_all'] = zip_info['county_names_all'].str.split('|')\n",
    "    zip_info['county'] = zip_info['county_name'] + ', ' + zip_info['state_id']\n",
    "\n",
    "    df = zip_info.copy()\n",
    "    df['cntys_found'] = np.isin(df['county'], bea.index)\n",
    "    # for ZIPs with an unfound county, try their second, third, and fourth listed counties if listed\n",
    "    for i in range(1,4):\n",
    "        county = []\n",
    "        for ind, row in df.iterrows():\n",
    "            if not row['cntys_found'] and len(row['county_names_all']) > i:\n",
    "                county.append(row['county_names_all'][i] + ', ' + row['state_id'])\n",
    "            else:\n",
    "                county.append(row['county'])\n",
    "        df['county'] = county\n",
    "        df['cntys_found'] = np.isin(df['county'], bea.index)\n",
    "\n",
    "    # for ZIPs with an unfound county or county with zero value, replace with the state's average\n",
    "    df['tw_compensation'] = [bea['Average Compensation'].loc[df['county'].iloc[i]] if df['cntys_found'].iloc[i] else np.nan for i in range(len(df))]\n",
    "    tw_sal = []\n",
    "    for ind, row in df.iterrows():\n",
    "        if not row['cntys_found'] or row['tw_compensation'] == 0:\n",
    "            tw_sal.append(df[df['state_id'] == row['state_id']]['tw_compensation'].mean())\n",
    "        else:\n",
    "            tw_sal.append(row['tw_compensation'])\n",
    "    df['tw_compensation'] = tw_sal\n",
    "\n",
    "    # min and max compensation\n",
    "    df['tw_compensation'] = np.where(df['tw_compensation'] < min_comp, min_comp, df['tw_compensation'])\n",
    "    df['tw_compensation'] = np.where(df['tw_compensation'] > max_comp, max_comp, df['tw_compensation'])\n",
    "\n",
    "    df[['zip', 'tw_compensation']].to_csv('tw_compensation_by_zip.csv', index=False)\n",
    "\n",
    "def make_invoice_item(cust_path=\"GT - Customer Dataset.csv\", item_path=\"GT - Item Dataset.csv\", sales_path=\"GT - Sales (Invoiced) Dataset.csv\", \n",
    "                cust_del=\"cust_del.xlsx\", cust_use_bill_addr=\"cust_use_bill_addr.xlsx\"):\n",
    "    make_invoice_dol(cust_path, item_path, sales_path, cust_del, cust_use_bill_addr, True)\n",
    "    \n",
    "def make_invoice_dol(cust_path=\"GT - Customer Dataset.csv\", item_path=\"GT - Item Dataset.csv\", sales_path=\"GT - Sales (Invoiced) Dataset.csv\", \n",
    "                cust_del=\"cust_del.xlsx\", cust_use_bill_addr=\"cust_use_bill_addr.xlsx\", output_item=False):\n",
    "    customer = pd.read_csv(cust_path, header=0, index_col=\"Entity ID\").drop_duplicates()\n",
    "    customer.drop(columns=['Address 3'], inplace=True)\n",
    "    item = pd.read_csv(item_path, header=0).drop_duplicates()\n",
    "    sales = pd.read_csv(sales_path, header=0).drop_duplicates()\n",
    "\n",
    "    if cust_del:\n",
    "        custs_d = pd.read_excel(cust_del).astype('str').values.flatten()\n",
    "        customer.drop(index=custs_d, errors='ignore', inplace=True)\n",
    "\n",
    "    if cust_use_bill_addr:\n",
    "        custs_b = pd.read_excel(cust_use_bill_addr).astype('str').values.flatten()\n",
    "        custs_b_df = customer.loc[custs_b]\n",
    "        custs_b_df[\"Address\"] = custs_b_df[\"Default Billing Address\"]\n",
    "        cust = customer.drop(index=custs_b, errors='ignore')\n",
    "        cust[\"Address\"] = cust[\"Default Shipping Address\"]\n",
    "        customer = pd.concat([custs_b_df, cust]).drop(columns=[\"Default Shipping Address\", \"Default Billing Address\"])\n",
    "        customer[\"Address\"] = customer[\"Address\"].astype(str)\n",
    "    else:\n",
    "        customer[\"Address\"] = customer[\"Default Shipping Address\"]\n",
    "\n",
    "    customer[\"Address\"] = customer[\"Address\"].str.removeprefix(\"Validate Address\\n\")\n",
    "\n",
    "    addr = []\n",
    "    for i in range(len(customer)):\n",
    "        splt = customer[\"Address\"].iloc[i].split(\"\\n\")\n",
    "        addr_i = list(np.empty(6, dtype=str))\n",
    "        for j in range(len(splt)):\n",
    "            addr_i[5-j] = splt[len(splt)-1-j]\n",
    "        addr.append(addr_i)\n",
    "\n",
    "    addr = np.array(addr)\n",
    "    # print(addr[addr[:,5] != \"United States\"])\n",
    "\n",
    "    customer[\"Zip Description\"] = addr[:,4]\n",
    "    customer[\"Address\"] = customer[\"Address\"].str.replace(\"\\n\", \" \")\n",
    "\n",
    "    # make customer the ID only\n",
    "    sales[\"Customer\"] = sales[\"Customer\"].apply(lambda x: str.split(x)[0])\n",
    "    # make amount a float\n",
    "    sales[\"Amount\"] = sales[\"Amount\"].str.replace(\"$\", \"\").str.replace(\",\", \"\").astype(float)\n",
    "\n",
    "    merged0 = pd.merge(sales, customer, how=\"left\", left_on=\"Customer\", right_on=\"Entity ID\")\n",
    "\n",
    "    merged = pd.merge(merged0, item, how=\"left\", left_on=\"Item\", right_on=\"Item Name/Number\")\n",
    "    merged.drop(columns=[\"Country\", \"Item Name/Number\", \"Carrier\", \"Parent\"], inplace=True)\n",
    "    merged.rename(columns={\"Customer\": \"Customer ID\", \"Company Name\": \"Customer Name\", \"Item Weight \": \"Item Weight\"}, inplace=True)    \n",
    "\n",
    "    if output_item:\n",
    "        # drop blank main lines\n",
    "        merged = merged[~(merged[\"Item\"].isna() & merged[\"Amount\"] == 0)]\n",
    "\n",
    "        merged.to_csv(\"invoice_items.csv\", index=False)\n",
    "    else:\n",
    "        merged.fillna(0, inplace=True)\n",
    "        gby = merged.groupby(by=[\"Date\", \"Customer ID\", \"Location\", \"Customer Name\", \"City\", \"State\", \"Zip\", \"Address\", \"Zip Description\"]).agg({'Amount': 'sum'})\n",
    "        gby.reset_index(inplace=True)\n",
    "        gby['Amount'] = gby['Amount'].round(2)\n",
    "        gby.to_csv(\"invoice_dol.csv\", index=False)\n",
    "\n",
    "def make_geo(lvl='zip2', zip_info_path='zip_census.csv', u_employ_path='u_employment.csv', out_path=''):\n",
    "    if lvl not in ['zip3', 'zip2']:\n",
    "        return\n",
    "    \n",
    "    zip_info = pd.read_csv(zip_info_path, dtype={'zip': str})\n",
    "    zip_info = zip_info[~zip_info['military']]\n",
    "    zip_info = zip_info[~zip_info['state_id'].isin(['AK', 'HI', 'DC','GU','PR','VI','MP','AS'])]\n",
    "\n",
    "    df = zip_info.copy()\n",
    "    df['zip2'] = df['zip'].str.slice(stop=2)\n",
    "    df['zip3'] = df['zip'].str.slice(stop=3)\n",
    "    df['county_weights'] = df['county_weights'].apply(lambda dict_str: list(eval(dict_str).values()))\n",
    "\n",
    "    ### Make a DataFrame of zip2/zip3s, their populations, and counties\n",
    "    data = dict()\n",
    "    z2_pop = []\n",
    "    cnty_weight = []\n",
    "    cnty_lists = []\n",
    "    # multi_st = 0 # count number of ZIPs with multiple states\n",
    "    for zip2 in df[lvl].unique():\n",
    "        z2_df = df[df[lvl] == zip2]\n",
    "        # if len(z2_df['state_id'].unique()) > 1:\n",
    "        #     multi_st += 1\n",
    "        counties = z2_df['county_names_all'].str.split('|')\n",
    "        cnty_list = sum(counties.values, [])\n",
    "        cnty_list = list(set(cnty_list))\n",
    "        cnty_pop = {c: 0 for c in cnty_list}\n",
    "        for ind in z2_df.index:\n",
    "            for i in range(len(counties.loc[ind])):\n",
    "                cnty_pop[counties.loc[ind][i]] += z2_df['county_weights'].loc[ind][i]/100 * z2_df['population'].loc[ind]\n",
    "        pop = z2_df['population'].sum()\n",
    "        z2_pop.append(pop)\n",
    "        cnty_st = {c: z2_df['state_id'].iloc[0] if not len(z2_df[z2_df['county_name'] == c]['state_id'])\n",
    "                else z2_df[z2_df['county_name'] == c]['state_id'].iloc[0] for c in cnty_list}\n",
    "        cnty_weight.append({c + ', ' + cnty_st[c]: cnty_pop[c] / pop for c in cnty_list})\n",
    "        cnty_lists.append([c + ', ' + cnty_st[c] for c in cnty_list])\n",
    "        cnty_pop = {c + ', ' + cnty_st[c]: cnty_pop[c] for c in cnty_pop}\n",
    "        data[zip2] = cnty_pop\n",
    "    z2 = pd.DataFrame({lvl: data.keys(), 'pop': z2_pop, 'counties': cnty_lists, 'county_pops': data.values(), 'county_weights': cnty_weight})\n",
    "\n",
    "    ### Make DataFrame of counties and total populations according to zip_info\n",
    "    county1 = np.concatenate([list(z2['county_pops'].iloc[i].keys()) for i in range(len(z2))])\n",
    "    pop1 =  np.concatenate([list(z2['county_pops'].iloc[i].values()) for i in range(len(z2))])\n",
    "    county_pop = pd.DataFrame({'county': county1, 'pop': pop1}).groupby(by='county').sum()\n",
    "\n",
    "    ### Add the percent of each county that is in the zip2/zip3\n",
    "    data = []\n",
    "    for i in range(z2.shape[0]):\n",
    "        dct = z2['county_pops'].iloc[i]\n",
    "        data.append({c: (dct[c]/county_pop.loc[c])[0] for c in dct})\n",
    "    z2['county_percents'] = data\n",
    "\n",
    "    ### Use the percent of each county inside the zip2/zip3 to aggregate the county-level\n",
    "    ###     u_employment values into one value for each zip2/zip3\n",
    "    bea = pd.read_csv(u_employ_path, index_col=0)\n",
    "\n",
    "    col = 'Number of Employees'\n",
    "    # bad_counties = set()\n",
    "    # zero_counties = set()\n",
    "    new_col = []\n",
    "    for i in range(len(z2)):\n",
    "        cntys = np.array(list(z2['county_percents'].iloc[i].keys()))\n",
    "        wghts = np.array(list(z2['county_percents'].iloc[i].values()))\n",
    "        cntys_found = np.isin(cntys, bea.index)\n",
    "        # print(wghts.shape[0], cntys_found.shape[0])\n",
    "        # bad_counties.update(cntys[~cntys_found])\n",
    "        cntys1 = cntys[cntys_found]\n",
    "        wghts1 = wghts[cntys_found] / wghts[cntys_found].sum()\n",
    "        # print(wghts1.shape[0], cntys1.shape[0])\n",
    "        # zero_counties.update(cntys1[cntys1==0])\n",
    "        u = np.array(bea[col].loc[cntys1].values).flatten()\n",
    "        u1 = np.where(u==0, 0.1, u)\n",
    "        new_col.append(np.dot(wghts1, u1))\n",
    "    z2['u_employ'] = new_col\n",
    "    # print(len(bad_counties), len(zero_counties), len(bea2))\n",
    "    \n",
    "    lats = []\n",
    "    lngs = []\n",
    "    for zip2 in df[lvl].unique():\n",
    "        z2_df = df[df[lvl] == zip2]\n",
    "        lats.append(np.dot(z2_df['population'], z2_df['lat'])/z2_df['population'].sum())\n",
    "        lngs.append(np.dot(z2_df['population'], z2_df['lng'])/z2_df['population'].sum())\n",
    "    z2['lat'] = lats\n",
    "    z2['lng'] = lngs\n",
    "\n",
    "    if not out_path:\n",
    "        out_path = lvl + '_geo.csv'\n",
    "    if out_path[-5] == '.xlsx':\n",
    "        z2.to_excel(out_path, index=False)\n",
    "    z2.to_csv(out_path, index=False)\n",
    "\n",
    "def make_geo_sales(lvl='zip2', geo_path='', sales_path='invoice_dol.csv', w_path='existing_locations.xlsx', features=['u_employ']):\n",
    "    if lvl not in ['zip3', 'zip2']:\n",
    "        return\n",
    "    if not geo_path:\n",
    "        geo_path = lvl + '_geo.csv'\n",
    "\n",
    "    if geo_path[-5:] == '.xlsx':\n",
    "        geo = pd.read_excel(geo_path, dtype={lvl: str})[[lvl, *features, 'lat','lng']]\n",
    "    else: # geo_path[-4:] == '.csv'\n",
    "        geo = pd.read_csv(geo_path, dtype={lvl: str})[[lvl, *features, 'lat','lng']]\n",
    "    \n",
    "    geo.dropna(subset=['lat', 'lng'], inplace=True)\n",
    "\n",
    "    if sales_path[-5:] == '.xlsx':\n",
    "        sales = pd.read_csv(sales_path, dtype={'Zip': str}, parse_dates=['Date'])\n",
    "    else: # sales_path[-4:] == '.csv'\n",
    "        sales = pd.read_csv(sales_path, dtype={'Zip': str}, parse_dates=['Date'])\n",
    "\n",
    "    # w_path[-4:] == '.csv'\n",
    "    ws = pd.read_excel(w_path, dtype={'zip': str}) # one row for each (warehouse, sales center) pair\n",
    "    w = ws.groupby('name').sum().reset_index().drop(columns=['sales_name']) # one row for each warehouse\n",
    "    \n",
    "    # aggregate dates to month-level\n",
    "    sales = pdh.daily_to_monthly(sales, date_col=\"Date\", cat_col=['Customer ID', 'State', 'Zip', 'Location'])\n",
    "    # filter to last 12 months of full data\n",
    "    sales = pdh.rolling_1yr(sales)\n",
    "\n",
    "    # change Location from column of Fulfillment Centers and Sales Centers to only Fulfillment Centers\n",
    "    warehouse_map = {ws['sales_name'].iloc[i]: ws['name'].iloc[i] for i in range(ws.shape[0])}\n",
    "    sales['warehouse'] = [warehouse_map[loc] if loc in warehouse_map.keys() else loc for loc in sales['Location']]\n",
    "\n",
    "    sales = sales[['date', 'warehouse', 'Customer ID', 'Zip', 'State', 'Amount']]\n",
    "    sales.rename(columns={'State': 'st'})\n",
    "    sales['zip3'] = sales['Zip'].str.slice(stop=3)\n",
    "    sales['zip2'] = sales['Zip'].str.slice(stop=2)\n",
    "\n",
    "    # group sales by geo lvl, warehouse, and date\n",
    "    df = sales.groupby([lvl, 'warehouse', 'date']).sum(numeric_only=True).reset_index()\n",
    "\n",
    "    # only include warehouses in w (drop FL)\n",
    "    df = df[np.isin(df['warehouse'], w['name'])]\n",
    "\n",
    "    # add rows so that all values of lvl in geo are in df, adding 0s for missing values\n",
    "    df = pdh.full_col(df, lvl, geo[lvl].unique())\n",
    "\n",
    "    # add rows so that all combinations of [lvl, 'warehouse', 'date] are in df\n",
    "    df = pdh.full_df(df, cat_col=[lvl, 'warehouse','date'])\n",
    "    df = df[(df['warehouse'] != \"0\") & (df['date'] != pd.to_datetime(0))]\n",
    "    df = df.sort_values(by=[lvl, 'warehouse', 'date']).reset_index(drop=True)\n",
    "\n",
    "    # merge geo, sales, w\n",
    "    w_coord = w[['name', 'lat', 'lng']]\n",
    "    df = pd.merge(left=df, right=w_coord, left_on='warehouse', right_on='name', how='left')\n",
    "    lvl_df = pd.merge(left=df, right=geo, on=lvl, suffixes=('_w', '_g'))\n",
    "    lvl_df1 = lvl_df[[lvl, 'warehouse', 'lat_w', 'lng_w', 'lat_g', 'lng_g']].drop_duplicates()\n",
    "\n",
    "    # calculate distances on the smaller version lvl_df1\n",
    "    lvl_df1['dist'] = [geodesic((lvl_df1['lat_g'].iloc[i], lvl_df1['lng_g'].iloc[i]), (lvl_df1['lat_w'].iloc[i], lvl_df1['lng_w'].iloc[i])).miles for i in range(lvl_df1.shape[0])]\n",
    "\n",
    "    # merge the distances back into the full lvl_df so the distances are repeated\n",
    "    lvl_df2 = pd.merge(left=lvl_df, right=lvl_df1[[lvl, 'warehouse', 'dist']], on=[lvl, 'warehouse']).drop(columns='name')\n",
    "\n",
    "    # calculate ms = $ sold / # utilities employees\n",
    "    lvl_df2['Amount'] = lvl_df2['Amount'].astype('float64').round(2)\n",
    "    lvl_df2['ms'] = lvl_df2['Amount'] / lvl_df2['u_employ']\n",
    "\n",
    "    lvl_df2.to_csv('%s_geo_sales.csv' % (lvl), index=False)\n",
    "\n",
    "### run functions\n",
    "# pull_bea()\n",
    "# make_geo()\n",
    "# make_tw_comp_by_zip()\n",
    "# # make_invoice_item()\n",
    "# make_invoice_dol()\n",
    "# make_geo_sales()\n",
    "# make_geo('zip3')\n",
    "# make_geo_sales('zip3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
